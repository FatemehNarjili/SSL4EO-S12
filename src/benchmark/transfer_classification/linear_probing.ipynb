{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f666d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "from torchgeo.models.vit import ViTLarge16_Weights, vit_large_patch16_224\n",
    "from datasets.EuroSat.eurosat_dataset import EurosatDataset,Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cvtorchvision import cvtransforms\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from torchgeo.datasets.eurosat import EuroSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('MAE linear probing for image classification', add_help=False)\n",
    "    parser.add_argument('--batch_size', default=512, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "    parser.add_argument('--epochs', default=90, type=int)\n",
    "    parser.add_argument('--accum_iter', default=1, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--model', default='vit_large_patch16', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                        help='weight decay (default: 0 for linear probe following MoCo v1)')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate (absolute lr)')\n",
    "\n",
    "    # * Finetuning params\n",
    "    parser.add_argument('--cls_token', action='store_false', dest='global_pool',\n",
    "                        help='Use class token instead of global pool for classification')\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_path', default='/datasets01/imagenet_full_size/061417/', type=str,\n",
    "                        help='dataset path')\n",
    "    parser.add_argument('--nb_classes', default=10, type=int,\n",
    "                        help='number of the classification types')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='./output_dir',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--log_dir', default='./output_dir',\n",
    "                        help='path where to tensorboard log')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--resume', default='',\n",
    "                        help='resume from checkpoint')\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true',\n",
    "                        help='Perform evaluation only')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--pin_mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "    parser.set_defaults(pin_mem=True)\n",
    "\n",
    "    \n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a32c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAND_STATS = {\n",
    "    'mean': [\n",
    "        1353.72696296, #'B01'\n",
    "        1117.20222222, #'B02' \n",
    "        1041.8842963,  #'B03' \n",
    "        946.554,       #'B04'\n",
    "        1199.18896296, #'B05' \n",
    "        2003.00696296, #'B06' \n",
    "        2374.00874074, #'B07' \n",
    "        2301.22014815, #'B08' \n",
    "        732.18207407,  #'B09' \n",
    "        12.09952894,   #'B10' \n",
    "        1820.69659259, #'B11' \n",
    "        1118.20259259,  #'B12' \n",
    "        2599.78311111, #'B8A'\n",
    "    ],\n",
    "    'std': [         \n",
    "         897.27143653,  #'B01'\n",
    "         736.01759721,  #'B02'\n",
    "         684.77615743,  #'B03'\n",
    "         620.02902871,  #'B04'\n",
    "         791.86263829,  #'B05'\n",
    "         1341.28018273, #'B06'\n",
    "         1595.39989386, #'B07'\n",
    "         1545.52915718, #'B08'\n",
    "         475.11595216,  #'B09'\n",
    "         98.26600935,   #'B10'\n",
    "         1216.48651476, #'B11'\n",
    "         736.6981037,    #'B12'\n",
    "         1750.12066835, #'B8A'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def main(args):\n",
    "    # Set up device\n",
    "    device = torch.device(args.device)\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.benchmark = True # note- comment in case of error for mps \n",
    "\n",
    "    # Create output directory\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = vit_large_patch16_224(weights=ViTLarge16_Weights.SENTINEL2_ALL_MAE)\n",
    "    model.head = nn.Linear(model.head.in_features, args.nb_classes)\n",
    "\n",
    "    model.head = torch.nn.Sequential(torch.nn.BatchNorm1d(model.head.in_features, affine=False, eps=1e-6), model.head)\n",
    "\n",
    "    # Initialize head parameters (e.g., Kaiming normal for weights, zeros for bias)\n",
    "    for m in model.head.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    # freeze all but the head\n",
    "    for _, p in model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "    for _, p in model.head.named_parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.weight_decay)\n",
    "\n",
    "   \n",
    "\n",
    "    # Load dataset\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=BAND_STATS['mean'], std=BAND_STATS['std']),\n",
    "    ])\n",
    "    \n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=BAND_STATS['mean'], std=BAND_STATS['std']),\n",
    "    ])\n",
    "\n",
    "    # Load EuroSAT dataset\n",
    "\n",
    "    train_dataset = EuroSAT(root=args.data_path, split='train', transform=transform_train)\n",
    "    val_dataset = EuroSAT(root=args.data_path, split='val', transform=transform_val)\n",
    "    test_dataset = EuroSAT(root=args.data_path, split='test', transform=transform_val)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                                   num_workers=args.num_workers, pin_memory=args.pin_mem)\n",
    "    val_loader = data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                                 num_workers=args.num_workers, pin_memory=args.pin_mem)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                                  num_workers=args.num_workers, pin_memory=args.pin_mem)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{args.epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4dd3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05ef63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d48af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468e05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl-venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
